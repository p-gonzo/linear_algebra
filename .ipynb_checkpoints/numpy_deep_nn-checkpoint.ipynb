{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for initilizing our nn weights and biasis\n",
    "def init_layers(nn_architecture, seed=101):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        curr_layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values[f\"W{curr_layer_idx}\"] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
    "        params_values[f\"b{curr_layer_idx}\"] = np.random.randn(layer_output_size) * 0.1\n",
    "        \n",
    "    return params_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions and their respective dirivatives:\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid_gradient(dA, Z):\n",
    "    # dA is the derivative of the current activation function\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_gradient(dA, Z):\n",
    "    # dA is the derivative of the current activation function\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    # Whichever values are less than or equal to zero have zero gradient\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Layer Forward Propagation:\n",
    "\n",
    "$$\\boldsymbol{Z}^{[l]} = \\boldsymbol{W}^{[l]} \\cdot \\boldsymbol{A}^{[l-1]} + \\boldsymbol{b}^{[l]}$$\n",
    "\n",
    "\n",
    "$$\\boldsymbol{A}^{[l]} = g^{[l]}(\\boldsymbol{Z}^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the inputs from a previous layer, calculate the next layer\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        active_fn = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        active_fn = sigmoid\n",
    "    else:\n",
    "        raise Exception(\"Non-supported activation function\")\n",
    "        \n",
    "    return active_fn(Z_curr), Z_curr # Why return both values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    # Create a temporary memory hash for backwards_prop\n",
    "    memory = {}\n",
    "    A_curr = X # X vector is the activation for layer 0\n",
    "    \n",
    "    # Iterate over network layers:\n",
    "    for prev_layer_idx, layer in enumerate(nn_architecture):\n",
    "        curr_layer_idx = prev_layer_idx + 1\n",
    "        # Transfer the previous activation into the current layer\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        curr_active_fn = layer[\"activation\"]\n",
    "        W_curr = params_values[f\"W{curr_layer_idx}\"]\n",
    "        b_curr = params_values[f\"b{curr_layer_idx}\"]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, curr_active_fn)\n",
    "        \n",
    "        # The activation of the current layer isn't stored in memory, b/c it is returned directly\n",
    "        memory[f\"A{prev_layer_idx}\"] = A_prev\n",
    "        memory[f\"Z{curr_layer_idx}\"] = Z_curr\n",
    "        \n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy Cost:\n",
    "![Cost Function](./assets/cost_function.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_cost(Y_hat, Y):\n",
    "    m = Y_hat.shape[1] # Number of examples\n",
    "    # https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#id11\n",
    "    # https://www.youtube.com/watch?v=mj5DpK5gGsY\n",
    "    cost = (-1 / m) * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used by NN, just for reporting\n",
    "def convert_prob_into_class(probs):\n",
    "    return np.array([float(prob > 0) for prob in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used by NN, just for reporting\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    _Y_hat = convert_prob_into_class(Y_hat)\n",
    "    return (Y == _Y_hat).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NN, we calculate the gradient of the cost function in respect to parameters, but backpropagation can be used to calculate derivatives of any function. The essence of this algorithm is the recursive use of a chain rule known from differential calculus: calculate a derivative of functions created by assembling other functions, whose derivatives we already know. This process - for one network layer - is described by the following formulas:\n",
    "\n",
    "$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} = \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n",
    "\n",
    "$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n",
    "\n",
    "$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n",
    "\n",
    "$$\\boldsymbol{dZ}^{[l]} = \\boldsymbol{dA}^{[l]} * g'(\\boldsymbol{Z}^{[l]})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backpropagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation):\n",
    "    # Number of examples:\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_fn = relu_gradient\n",
    "    elif activiation is \"sigmoid\":\n",
    "        activation_fn = sigmoid_gradient\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # The gradient Z with respect to the current ativiation\n",
    "    dZ_curr = activation_fn(dA_curr, Z_curr)\n",
    "    \n",
    "    # Partial derivative matrix W with respect to Loss\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    # Partial derivative of vector b with respect to loss\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # Partial deriviative of the previous layer's activatino func with respect to the loss\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "    \n",
    "    return dA_prev, dW_curr, db_curr  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by calculating a derivative of the cost function with respect to the prediction vector: the result of forward propagation. This is quite trivial as it only consists of rewriting the following formula. Then iterate through the layers of the network starting from the end and calculate the derivatives with respect to all parameters. Ultimately, function returns a python dictionary containing the gradient we are looking for:\n",
    "\n",
    "$$\\frac{\\partial L }{\\partial \\boldsymbol{\\hat{Y}}} = -(\\frac{\\boldsymbol{Y}}{\\boldsymbol{\\hat{Y}}}- \\frac{1-\\boldsymbol{Y}}{1-\\boldsymbol{\\hat{Y}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backpropagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    \n",
    "    # Store our gradients in a dictionary for later use\n",
    "    grads_values = {}\n",
    "    \n",
    "    # Number of samples\n",
    "    m = Y.shape[0]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # Derivative of the cost function with respect to the prediction vector\n",
    "    dA_prev = -1 * (np.divide(Y, Y_hat) - np.divide((1 - Y), (1 - Y_hat)))\n",
    "    \n",
    "    for prev_layer_idx, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        curr_layer_idx = prev_layer_idx + 1\n",
    "        curr_activation_fn = layer['activation']\n",
    "\n",
    "        dA_curr = dA_prev\n",
    "\n",
    "        A_prev = memory[f\"A{prev_layer_idx}\"]\n",
    "        Z_curr = memory[f\"Z{curr_layer_idx}\"]\n",
    "        W_curr = params_values[f\"W{curr_layer_idx}\"]\n",
    "        b_curr = params_values[f\"b{curr_layer_idx}\"]\n",
    "\n",
    "        dA_prev, dW_curr, db_curr = single_layer_backpropagation(\n",
    "            dA_curr,\n",
    "            W_curr,\n",
    "            b_curr,\n",
    "            Z_curr,\n",
    "            A_prev,\n",
    "            curr_activation_fn\n",
    "        )\n",
    "\n",
    "        # Store our gradients in a dictionary for later updates\n",
    "        grads_values[f\"dW{curr_layer_idx}\"] = dW_curr\n",
    "        grads_values[f\"db{curr_layer_idx}\"] = db_curr\n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_values = init_layers(NN_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 2.70684984e-01,  6.28132709e-02],\n",
       "        [ 9.07969446e-02,  5.03825754e-02],\n",
       "        [ 6.51117948e-02, -3.19318045e-02],\n",
       "        [-8.48076983e-02,  6.05965349e-02],\n",
       "        [-2.01816824e-01,  7.40122057e-02],\n",
       "        [ 5.28813494e-02, -5.89000533e-02],\n",
       "        [ 1.88695309e-02, -7.58872056e-02],\n",
       "        [-9.33237216e-02,  9.55056509e-02],\n",
       "        [ 1.90794322e-02,  1.97875732e-01],\n",
       "        [ 2.60596728e-01,  6.83508886e-02],\n",
       "        [ 3.02665449e-02,  1.69372293e-01],\n",
       "        [-1.70608593e-01, -1.15911942e-01],\n",
       "        [-1.34840721e-02,  3.90527843e-02],\n",
       "        [ 1.66904636e-02,  1.84501859e-02],\n",
       "        [ 8.07705914e-02,  7.29596753e-03],\n",
       "        [ 6.38787013e-02,  3.29646299e-02],\n",
       "        [-4.97104023e-02, -7.54069701e-02],\n",
       "        [-9.43406403e-02,  4.84751647e-02],\n",
       "        [-1.16773316e-02,  1.90175480e-01],\n",
       "        [ 2.38126959e-02,  1.99665229e-01],\n",
       "        [-9.93263500e-02,  1.96799505e-02],\n",
       "        [-1.13664459e-01,  3.66479606e-05],\n",
       "        [ 1.02598415e-01, -1.56597904e-02],\n",
       "        [-3.15791439e-03,  6.49825833e-02],\n",
       "        [ 2.15484644e-01, -6.10258856e-02]]),\n",
       " 'b1': array([-0.07553253, -0.03464185,  0.01470268, -0.0479448 ,  0.05587694,\n",
       "         0.10248103, -0.09258743,  0.18628641, -0.11338172,  0.06104779,\n",
       "         0.03860303,  0.20840185, -0.03765187,  0.02303363,  0.06812093,\n",
       "         0.10351251, -0.00311605,  0.19399323, -0.10051869, -0.07417897,\n",
       "         0.01871245, -0.07328451, -0.13829201,  0.14824955,  0.09614582]),\n",
       " 'W2': array([[-0.21412123,  0.09925735,  0.11922406, ..., -0.0866885 ,\n",
       "          0.07207876, -0.1223082 ],\n",
       "        [ 0.16067799, -0.111571  , -0.13853786, ...,  0.06775351,\n",
       "          0.00261055, -0.16782837],\n",
       "        [ 0.03339731, -0.05324705,  0.21177274, ...,  0.01962753,\n",
       "         -0.0982776 ,  0.22315552],\n",
       "        ...,\n",
       "        [ 0.02492407,  0.15403748, -0.20465305, ...,  0.11254965,\n",
       "          0.18676182,  0.07063028],\n",
       "        [-0.05494316,  0.14727726, -0.09734215, ..., -0.04047882,\n",
       "          0.01738749,  0.12348664],\n",
       "        [-0.23873587,  0.06862739,  0.11557958, ..., -0.01572931,\n",
       "          0.07672195, -0.03214205]]),\n",
       " 'b2': array([ 0.02460016, -0.02724128, -0.0820951 ,  0.04297878, -0.07731835,\n",
       "         0.09918309,  0.06822797, -0.07875683, -0.07534387,  0.12314289,\n",
       "         0.10812576, -0.03735697, -0.017648  , -0.03056806,  0.13797275,\n",
       "         0.0814899 ,  0.04865042, -0.09090215, -0.10549663,  0.09393058,\n",
       "         0.16695412, -0.27272496, -0.16027488,  0.06367029, -0.02431533,\n",
       "        -0.09455949,  0.06783836, -0.13294034,  0.09037498, -0.04379278,\n",
       "         0.16558465, -0.02047427,  0.09734175, -0.0455153 , -0.09799584,\n",
       "        -0.0598696 ,  0.00606744, -0.15355042,  0.11068974, -0.00291085,\n",
       "        -0.23351087, -0.1486499 , -0.11418288, -0.11165352,  0.01887587,\n",
       "        -0.01977726,  0.03936644, -0.07202395,  0.09638343, -0.0701214 ]),\n",
       " 'W3': array([[-0.0817911 ,  0.04629911, -0.10157381, ..., -0.00849843,\n",
       "          0.04804257,  0.11290356],\n",
       "        [ 0.02662868,  0.0172699 , -0.01180884, ...,  0.1191246 ,\n",
       "         -0.01941874, -0.20148411],\n",
       "        [-0.04295685, -0.05073441,  0.01467874, ...,  0.13385629,\n",
       "          0.07701295,  0.08297073],\n",
       "        ...,\n",
       "        [ 0.11914214,  0.11011301,  0.0483402 , ..., -0.05822715,\n",
       "          0.1220733 ,  0.14577231],\n",
       "        [ 0.03192726, -0.04663341,  0.06323958, ...,  0.17978001,\n",
       "          0.10491881, -0.09207605],\n",
       "        [-0.02306655,  0.02961225,  0.00555922, ...,  0.06322523,\n",
       "          0.05271541,  0.12095801]]),\n",
       " 'b3': array([ 0.05949661, -0.05413871,  0.057954  ,  0.09106455, -0.15637928,\n",
       "         0.07981025,  0.02955265, -0.04886772, -0.03109361,  0.00144835,\n",
       "         0.03830699,  0.07080944, -0.06241705,  0.11945765,  0.06433496,\n",
       "         0.00339333,  0.02600773, -0.10545933,  0.1028586 ,  0.1248504 ,\n",
       "        -0.10244212,  0.06845128,  0.13279375, -0.10572901, -0.04743093,\n",
       "         0.0500842 ,  0.10349476, -0.22244443, -0.01709271, -0.04145612,\n",
       "        -0.11715689,  0.06614131, -0.02818438, -0.28073389,  0.02543365,\n",
       "         0.08349985, -0.0393556 , -0.05977684, -0.07603892,  0.00949198,\n",
       "        -0.10357951,  0.0782356 , -0.00336118, -0.03131481, -0.10820305,\n",
       "        -0.05616306, -0.2801442 ,  0.03685175,  0.0682194 , -0.0604056 ]),\n",
       " 'W4': array([[ 0.18710691,  0.00936497,  0.0394169 , ..., -0.16679635,\n",
       "         -0.04990443, -0.03501606],\n",
       "        [-0.13473199,  0.00287586, -0.0524538 , ..., -0.00787059,\n",
       "         -0.05645397,  0.10108878],\n",
       "        [-0.11689725,  0.19502651,  0.13036394, ...,  0.15025343,\n",
       "         -0.00193913, -0.01986325],\n",
       "        ...,\n",
       "        [ 0.13940237, -0.05545267, -0.17631563, ..., -0.16137949,\n",
       "          0.14673309,  0.01607781],\n",
       "        [ 0.09401443,  0.0209783 , -0.15149879, ..., -0.08687996,\n",
       "          0.05035102, -0.20883282],\n",
       "        [-0.11281302,  0.0445701 ,  0.11322291, ...,  0.01679945,\n",
       "          0.09991765, -0.04267053]]),\n",
       " 'b4': array([ 0.00983968,  0.0499768 ,  0.05391909,  0.05978266, -0.11705552,\n",
       "         0.07058229,  0.21540126,  0.05411339,  0.10181001,  0.03416985,\n",
       "        -0.18170346, -0.02064487, -0.0242427 ,  0.01334918, -0.06616004,\n",
       "        -0.15197536, -0.02758863, -0.05281354,  0.07491712,  0.09044389,\n",
       "         0.01667413,  0.20622115, -0.01862505, -0.05317595,  0.21081661]),\n",
       " 'W5': array([[ 0.04434335, -0.05473137, -0.01036881, -0.02097658, -0.09423221,\n",
       "          0.17561507, -0.0198446 ,  0.09119393, -0.07583376,  0.06164873,\n",
       "          0.13707095,  0.03325029,  0.1299057 ,  0.01701341,  0.06281011,\n",
       "         -0.01033877,  0.01134803, -0.01868185, -0.02337042,  0.07274475,\n",
       "          0.04582195,  0.08700944,  0.0396944 ,  0.15289012,  0.04229262]]),\n",
       " 'b5': array([-0.03397778])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_curr, memory = full_forward_propagation(np.array([1,1]), params_values, NN_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A0': array([1, 1]),\n",
       " 'Z1': array([ 0.25796572,  0.10653767,  0.04788267, -0.07215597, -0.07192768,\n",
       "         0.09646232, -0.1496051 ,  0.18846834,  0.10357345,  0.38999541,\n",
       "         0.23824187, -0.07811868, -0.01208316,  0.05817428,  0.15618749,\n",
       "         0.20035584, -0.12823342,  0.14812776,  0.07797946,  0.14929895,\n",
       "        -0.06093395, -0.18691233, -0.05135339,  0.21007422,  0.25060457]),\n",
       " 'A1': array([0.25796572, 0.10653767, 0.04788267, 0.        , 0.        ,\n",
       "        0.09646232, 0.        , 0.18846834, 0.10357345, 0.38999541,\n",
       "        0.23824187, 0.        , 0.        , 0.05817428, 0.15618749,\n",
       "        0.20035584, 0.        , 0.14812776, 0.07797946, 0.14929895,\n",
       "        0.        , 0.        , 0.        , 0.21007422, 0.25060457]),\n",
       " 'Z2': array([-0.04077838, -0.01644601, -0.01304237, -0.00952487, -0.16221483,\n",
       "         0.08872989,  0.03257746, -0.03374251, -0.0765148 ,  0.21248075,\n",
       "         0.18171438, -0.17162753,  0.04408912,  0.05183725,  0.20597302,\n",
       "        -0.07470464,  0.00444537, -0.07375935, -0.18421638,  0.02229237,\n",
       "         0.17252069, -0.21572042, -0.20315106,  0.11686339,  0.03081345,\n",
       "        -0.03871089, -0.15695227, -0.20138929,  0.10575886, -0.0327001 ,\n",
       "         0.23346352, -0.03246335,  0.03403241, -0.07804522,  0.01195491,\n",
       "         0.02013289,  0.07539384, -0.20076149,  0.07731092,  0.02667314,\n",
       "        -0.33715852, -0.14210548, -0.18274903, -0.151911  ,  0.04524079,\n",
       "         0.02738857, -0.01719261,  0.03307041,  0.1839941 , -0.08232441]),\n",
       " 'A2': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08872989, 0.03257746, 0.        , 0.        , 0.21248075,\n",
       "        0.18171438, 0.        , 0.04408912, 0.05183725, 0.20597302,\n",
       "        0.        , 0.00444537, 0.        , 0.        , 0.02229237,\n",
       "        0.17252069, 0.        , 0.        , 0.11686339, 0.03081345,\n",
       "        0.        , 0.        , 0.        , 0.10575886, 0.        ,\n",
       "        0.23346352, 0.        , 0.03403241, 0.        , 0.01195491,\n",
       "        0.02013289, 0.07539384, 0.        , 0.07731092, 0.02667314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04524079,\n",
       "        0.02738857, 0.        , 0.03307041, 0.1839941 , 0.        ]),\n",
       " 'Z3': array([ 0.04618317,  0.00152305,  0.03000939,  0.10739465, -0.18939302,\n",
       "         0.08515588,  0.0597655 ,  0.01099781, -0.04810476, -0.01699644,\n",
       "         0.07244282,  0.07610788, -0.09037795,  0.10056853,  0.01397258,\n",
       "         0.02088121,  0.04911103, -0.13325914,  0.13797528,  0.19475299,\n",
       "        -0.09190346,  0.12492672,  0.12709555, -0.12875092,  0.06079318,\n",
       "         0.09550016,  0.13726045, -0.31210598,  0.08652621, -0.03258115,\n",
       "        -0.10746415, -0.01669646,  0.01228118, -0.30484801, -0.08290447,\n",
       "         0.00864843, -0.11742508, -0.03130422, -0.1605046 , -0.02186444,\n",
       "         0.05347576,  0.13289956, -0.03621247, -0.05569148, -0.15005551,\n",
       "        -0.04866845, -0.30031493,  0.05359892,  0.12482349, -0.0936098 ]),\n",
       " 'A3': array([0.04618317, 0.00152305, 0.03000939, 0.10739465, 0.        ,\n",
       "        0.08515588, 0.0597655 , 0.01099781, 0.        , 0.        ,\n",
       "        0.07244282, 0.07610788, 0.        , 0.10056853, 0.01397258,\n",
       "        0.02088121, 0.04911103, 0.        , 0.13797528, 0.19475299,\n",
       "        0.        , 0.12492672, 0.12709555, 0.        , 0.06079318,\n",
       "        0.09550016, 0.13726045, 0.        , 0.08652621, 0.        ,\n",
       "        0.        , 0.        , 0.01228118, 0.        , 0.        ,\n",
       "        0.00864843, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05347576, 0.13289956, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.05359892, 0.12482349, 0.        ]),\n",
       " 'Z4': array([ 0.03130761,  0.08827948, -0.0156533 ,  0.07947365, -0.10202502,\n",
       "         0.12994665,  0.1523095 ,  0.08939261,  0.11219403,  0.04171969,\n",
       "        -0.21517159, -0.02810781, -0.12858434, -0.01037783, -0.05934049,\n",
       "        -0.10033177, -0.05098204, -0.00978597,  0.10699364,  0.05285691,\n",
       "         0.02022215,  0.22033267,  0.02931036,  0.03484607,  0.21244361]),\n",
       " 'A4': array([0.03130761, 0.08827948, 0.        , 0.07947365, 0.        ,\n",
       "        0.12994665, 0.1523095 , 0.08939261, 0.11219403, 0.04171969,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10699364, 0.05285691,\n",
       "        0.02022215, 0.22033267, 0.02931036, 0.03484607, 0.21244361]),\n",
       " 'Z5': array([0.01984386])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation of the current layer isn't stored in memory, b/c it is returned directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_curr == sigmoid(memory['Z5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
